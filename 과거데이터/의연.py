import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import pandas as pd
import numpy as np
import glob

# 1. í•©ì¹  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì˜ˆ: 'data_2025_10_22.xls', 'data_2025_10_23.xls' ë“±)
# globëŠ” í˜„ì¬ í´ë” ë‚´ì˜ ëª¨ë“  .xls íŒŒì¼ì„ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
file_list = glob.glob('test (*).xls')

# 2. ê° íŒŒì¼ì„ ì½ì–´ ë°ì´í„°í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
all_data = []
for file in file_list:
    # ì—‘ì…€ì˜ 17í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ 16ì¤„ ê±´ë„ˆë›°ê¸°
    df = pd.read_excel(file, skiprows=16) 
    
    # 'ê³„' ì»¬ëŸ¼ì˜ ê°’ë§Œ ì¶”ì¶œ
    data_values = df['ê³„'].values.astype('float32')
    
    # NaN(ê²°ì¸¡ì¹˜) ì œê±°
    data_values = data_values[~np.isnan(data_values)]
    
    # ğŸŒŸğŸŒŸğŸŒŸ ê°€ì¥ ì¤‘ìš”: ê° íŒŒì¼ì˜ ë§¨ ì•„ë˜ 'ì´í•©ê³„' í–‰ì„ ì œê±° ğŸŒŸğŸŒŸğŸŒŸ
    # ìœ íš¨í•œ ì‹œê°„ëŒ€ ë°ì´í„° 24ê°œë§Œ ë‚¨ê¸°ê¸° ìœ„í•´ ë§ˆì§€ë§‰ í–‰ ì œê±°
    if len(data_values) > 24:
        data_values = data_values[:-1]
    
    # ë°ì´í„°í”„ë ˆì„ í˜•íƒœë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (concatì„ ìœ„í•´)
    all_data.append(pd.DataFrame(data_values, columns=['ê³„']))

# 3. ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ í–‰(axis=0) ë°©í–¥ìœ¼ë¡œ í•©ì¹˜ê¸°
combined_df = pd.concat(all_data, axis=0, ignore_index=True)

# 4. í•©ì³ì§„ ë°ì´í„°ë¥¼ ìµœì¢… 'data' ë³€ìˆ˜ì— í• ë‹¹
data = combined_df['ê³„'].values.astype('float32')
data = data.reshape(-1, 1)

## ----------------------------------------------------
## 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ê·œí™”
## ----------------------------------------------------
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§: LSTM ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ 0ê³¼ 1 ì‚¬ì´ë¡œ ì •ê·œí™”
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
train_size = int(len(scaled_data) * 0.5)
train_data = scaled_data[:train_size]
test_data = scaled_data[train_size:]

# ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜
def create_dataset(dataset, look_back=1):
    X, Y = [], []
    for i in range(len(dataset) - look_back):
        # i ì‹œì ë¶€í„° look_back ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥(X)ìœ¼ë¡œ ì‚¬ìš©
        a = dataset[i:(i + look_back), 0]
        X.append(a)
        # i + look_back ì‹œì ì˜ ê°’ì„ ì¶œë ¥(Y)ìœ¼ë¡œ ì‚¬ìš© (ë‹¤ìŒ ì‹œì  ì˜ˆì¸¡)
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

# look_back(ê³¼ê±° ëª‡ ì‹œì ì„ ë³¼ì§€) ì„¤ì •
look_back = 3 
X_train, Y_train = create_dataset(train_data, look_back)
X_test, Y_test = create_dataset(test_data, look_back)

# LSTM ì…ë ¥ í˜•íƒœì— ë§ê²Œ ë°ì´í„° ì°¨ì› ë³€í™˜ (Samples, Timesteps, Features)
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

## ----------------------------------------------------
## 3. LSTM ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
## ----------------------------------------------------
model = Sequential()
# LSTM ë ˆì´ì–´ ì¶”ê°€ (50ì€ LSTM ìœ ë‹›ì˜ ê°œìˆ˜)
model.add(LSTM(50, input_shape=(look_back, 1)))
# ì¶œë ¥ ë ˆì´ì–´ (íšŒê·€ ì˜ˆì¸¡ì´ë¯€ë¡œ 1ê°œì˜ ë‰´ëŸ°)
model.add(Dense(1))

# ëª¨ë¸ ì»´íŒŒì¼ (ìµœì í™” í•¨ìˆ˜: adam, ì†ì‹¤ í•¨ìˆ˜: MSE)
model.compile(optimizer='adam', loss='mean_squared_error')

# ëª¨ë¸ í•™ìŠµ
print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=0)
print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

## ----------------------------------------------------
## 4. ì˜ˆì¸¡ ë° ê²°ê³¼ ì‹œê°í™”
## ----------------------------------------------------
# ì˜ˆì¸¡ ìˆ˜í–‰
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# ì •ê·œí™”ëœ ê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë˜ëŒë¦¬ê¸° (Inverse Transform)
train_predict = scaler.inverse_transform(train_predict)
Y_train_original = scaler.inverse_transform(Y_train.reshape(-1, 1))
test_predict = scaler.inverse_transform(test_predict)
Y_test_original = scaler.inverse_transform(Y_test.reshape(-1, 1))
data_original = scaler.inverse_transform(scaled_data)

# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.plot(data_original, label='Original Data (True Congestion)')

# í•™ìŠµ ì˜ˆì¸¡ ê²°ê³¼ í”Œë¡¯ (look_backë§Œí¼ ë°€ë ¤ì„œ ì‹œì‘)
train_plot = np.empty_like(data_original)
train_plot[:, :] = np.nan
train_plot[look_back:len(train_predict) + look_back, :] = train_predict
plt.plot(train_plot, label='Train Prediction')

# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ í”Œë¡¯
test_plot = np.empty_like(data_original)
test_plot[:, :] = np.nan
# **ìˆ˜ì •ëœ ë¶€ë¶„:** í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹œì‘ ìœ„ì¹˜ë¥¼ 'í•™ìŠµ ë°ì´í„° ê¸¸ì´ + look_back'ìœ¼ë¡œ ì •í™•íˆ ë§ì¶¥ë‹ˆë‹¤.
# look_backì„ 3ìœ¼ë¡œ ì„¤ì •í•˜ì…¨ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# í•™ìŠµ ë°ì´í„°ê°€ ëë‚˜ëŠ” ì‹œì ì€ len(train_predict) + look_back ì…ë‹ˆë‹¤.
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì´ ì‹œì ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
test_start_index = len(train_predict) + look_back

# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ì‚½ì…
test_plot[test_start_index:test_start_index + len(test_predict), :] = test_predict

plt.plot(test_plot, label='Test Prediction')

plt.title('LSTM Congestion Prediction Example')
plt.xlabel('Time Step')
plt.ylabel('Congestion Level')
plt.legend()
plt.show()

# ì˜ˆì¸¡ ì •í™•ë„ í‰ê°€ (RMSE)
from sklearn.metrics import mean_squared_error
train_rmse = np.sqrt(mean_squared_error(Y_train_original, train_predict))
test_rmse = np.sqrt(mean_squared_error(Y_test_original, test_predict))
print(f"Train RMSE: {train_rmse:.4f}")
print(f"Test RMSE: {test_rmse:.4f}")